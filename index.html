<!DOCTYPE html>

<html lang="en" data-theme="light">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Jang Oh Tae | Personal Page</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="style.css">
</head>
<body>

<div class="container">
    <aside class="sidebar">
        <div class="profile-picture">
            <img src="https://avatars.githubusercontent.com/u/136439287?v=4" alt="Jang Oh Tae">            </div>
        <h1>Jang Oh Tae</h1>
        <p class="affiliation">
            MS-PhD Student<br>
            <a href="http://iras.postech.ac.kr/main/index.php" target="_blank" rel="noopener noreferrer">Intelligent Radar Signal Processing Lab</a><br>
            Dept. of Electronic Engineering<br>
            <a href="https://www.postech.ac.kr/kor/index.do" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a><br>
            Supervisor: <a href="http://iras.postech.ac.kr/main/bbs/bbs/board.php?bo_table=professor&wr_id=1" target="_blank" rel="noopener noreferrer">Professor Kyung-Tae Kim</a>   
            <br> I am working on SAR imaging<br>
        </p>
        <div class="social-links">
<a href="https://github.com/otaejang" aria-label="GitHub" title="GitHub">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg>
            </a>
            <a href="#" aria-label="LinkedIn" title="LinkedIn">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg>
            </a>
            <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=wHzhF4kAAAAJ" aria-label="Google Scholar" title="Google Scholar">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M5.242 13.769L0 9.5L12 0l12 9.5l-5.242 4.269C17.548 11.249 14.978 9 12 9s-5.548 2.249-6.758 4.769zM12 10c-3.313 0-6 2.686-6 6s2.687 6 6 6s6-2.686 6-6s-2.687-6-6-6z"/></svg>
            </a>
            <a href="#" aria-label="X" title="X">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
            </a>
            <a href="path/to/your/cv.pdf" aria-label="Download CV" title="Download CV">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
            </a>
        </div>
    </aside>
    <main class="main-content">
    <div class="header">
        <h2>Papers</h2>
        <button id="theme-toggle" aria-label="Toggle theme">
            <svg class="sun" xmlns="[http://www.w3.org/2000/svg](http://www.w3.org/2000/svg)" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg>
            <svg class="moon" xmlns="[http://www.w3.org/2000/svg](http://www.w3.org/2000/svg)" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>
        </button>
    </div>
    <div class="papers-list">
        <div class="paper-item">
            <div class="paper-info">
                <h3>
                    <a href="https://ieeexplore.ieee.org/abstract/document/11080270/" target="_blank" rel="noopener noreferrer" class="paper-link">IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized SAR-ATR</a>
                </h3>
                <p>Authors: Oh-Tae Jang; Min-Jun Kim; Sung-Ho Kim; Hee-Sub Shin; Kyung-Tae Kim</p>
                <p><em>IEEE Transactions on Aerospace and Electronic Systems, 2025</em></p>
                <button class="see-more-btn">See More</button>
                <div class="abstract hidden">
                    <p>Recently, computer-aided design models and electromagnetic simulations have been used to augment synthetic aperture radar (SAR) data for deep learning. However, an automatic target recognition (ATR) model struggles with domain shift when using synthetic data because the model learns specific clutter patterns present in such data, which disturbs performance when applied to measured data with different clutter distributions. This study proposes IRASNet, a domain-generalized SAR-ATR framework designed to achieve effective feature-level clutter reduction and domain-invariant feature learning. The proposed framework introduces a clutter reduction module (CRM) that enhances the signal-to-clutter ratio on feature maps, mitigating the impact of clutter while preserving essential target and shadow information to improve ATR performance. To further enhance generalization, adversarial learning is integrated with CRM to extract clutter-reduced domain-invariant features, bridging the gap between synthetic and measured datasets without requiring measured data during training. Additionally, a positional supervision task is introduced using mask-based ground truth encoding to improve feature extraction from target and shadow regions, strengthening the model's class discrimination capability. Our proposed IRASNet not only enhances generalization performance but also significantly improves feature-level clutter reduction, making it a valuable advancement in the field of radar image pattern recognition.
</p>
                </div>
            </div>
            <img src="./paper1.png" alt="Paper Preview" class="paper-preview">
        </div>
        <div class="paper-item">
            <div class="paper-info">
                <h3>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10763519/" target="_blank" rel="noopener noreferrer" class="paper-link">Soft Segmented Randomization: Enhancing Domain Generalization in SAR ATR for Synthetic-to-Measured</a>
                </h3>
                <p>Authors: Minjun Kim, Ohtae Jang, Haekang Song, Heesub Shin, Jaewoo Ok, Minyoung Back, Jaehyuk Youn, Sungho Kim</p>
                <p><em>IEEE Access, 2024</em></p>
                <button class="see-more-btn">See More</button>
                <div class="abstract hidden">
                    <p>Synthetic aperture radar technology is crucial for high-resolution imaging under various conditions; however, the acquisition of real-world synthetic aperture radar data for deep learning-based automatic target recognition remains challenging due to high costs and data availability issues. To overcome these challenges, synthetic data generated through simulations have been employed, although discrepancies between synthetic and real data—stemming from factors such as background clutter and target signature differences—can degrade model performance. In this study, we introduce a novel framework, soft segmented randomization, designed to reduce domain discrepancy and improve the generalization of synthetic aperture radar automatic target recognition models. The soft segmented randomization framework applies a Gaussian mixture model to segment target and clutter regions softly, introducing randomized variations that align the synthetic data’s statistical properties more closely with those of real-world data. Experimental results demonstrate that the proposed soft segmented randomization framework significantly enhances model performance on measured synthetic aperture radar data, making it a promising approach for robust automatic target recognition in scenarios with limited or no access to measured data.

                    </p>
                </div>
            </div>
            <img src="./paper2.png" alt="Paper Preview" class="paper-preview">
        </div>
        <div class="paper-item">
            <div class="paper-info">
                <h3>
                    <a href="https://www.mdpi.com/1424-8220/24/7/2286" target="_blank" rel="noopener noreferrer" class="paper-link">S-LIGHT: Synthetic Dataset for the Separation of Diffuse and Specular Reflection Images</a>
                </h3>
                <p>Authors: Sangho Jo, Ohtae Jang, Chaitali Bhattacharyya, Minjun Kim, Taeseok Lee, Yewon Jang, Haekang Song, Hyukmin Kwon, Saebyeol Do, Sungho Kim</p>
                <p><em>Sensors, 2024</em></p>
                <button class="see-more-btn">See More</button>
                <div class="abstract hidden">
                    <p>Several studies in computer vision have examined specular removal, which is crucial for object detection and recognition. This research has traditionally been divided into two tasks: specular highlight removal, which focuses on removing specular highlights on object surfaces, and reflection removal, which deals with specular reflections occurring on glass surfaces. In reality, however, both types of specular effects often coexist, making it a fundamental challenge that has not been adequately addressed. Recognizing the necessity of integrating specular components handled in both tasks, we constructed a specular-light (S-Light) DB for training single-image-based deep learning models. Moreover, considering the absence of benchmark datasets for quantitative evaluation, the multi-scale normalized cross correlation (MS-NCC) metric, which considers the correlation between specular and diffuse components, was introduced to assess the learning outcomes.</p>
                </div>
            </div>
            <img src="./paper3.png" alt="Paper Preview" class="paper-preview">
        </div>
    </div>
</main>


<script src="script.js"></script>

</body>
</html>